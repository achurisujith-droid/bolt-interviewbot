import React, { useState, useEffect, useRef } from 'react';
import { Video, Mic, MicOff, Camera, CameraOff, Phone, PhoneOff, Users, Clock, MessageSquare, Volume2, Eye, Monitor } from 'lucide-react';
import { io, Socket } from 'socket.io-client';

interface VideoInterviewAgentProps {
  sessionId: string;
  candidateName: string;
  position: string;
  onComplete: (responses: any[]) => void;
}

export const VideoInterviewAgent: React.FC<VideoInterviewAgentProps> = ({
  sessionId,
  candidateName,
  position,
  onComplete
}) => {
  const [isConnected, setIsConnected] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [currentQuestion, setCurrentQuestion] = useState(0);
  const [aiSpeaking, setAiSpeaking] = useState(false);
  const [candidateResponding, setCandidateResponding] = useState(false);
  const [waitingForResponse, setWaitingForResponse] = useState(false);
  const [connectionStatus, setConnectionStatus] = useState<'connecting' | 'connected' | 'disconnected'>('connecting');
  const [currentQuestionText, setCurrentQuestionText] = useState('');
  const [responses, setResponses] = useState<any[]>([]);
  const [overallScore, setOverallScore] = useState<number | null>(null);
  const [isVideoEnabled, setIsVideoEnabled] = useState(true);
  const [isScreenSharing, setIsScreenSharing] = useState(false);
  const [videoAnalysis, setVideoAnalysis] = useState<string>('');
  const [interviewStarted, setInterviewStarted] = useState(false);
  const [interviewEnded, setInterviewEnded] = useState(false);
  const [questionInProgress, setQuestionInProgress] = useState(false);
  const [waitingForAnswer, setWaitingForAnswer] = useState(false);
  
  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const videoRecorderRef = useRef<MediaRecorder | null>(null);
  const socketRef = useRef<Socket | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const videoChunksRef = useRef<Blob[]>([]);

  // Initialize connection
  useEffect(() => {
    initializeConnection();
    return () => {
      cleanup();
    };
  }, []);

  const initializeConnection = async () => {
    try {
      // Get user media (camera + microphone)
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { 
          width: 1280, 
          height: 720,
          frameRate: 30,
          facingMode: 'user'
        },
        audio: { echoCancellation: true, noiseSuppression: true }
      });

      if (videoRef.current) {
        videoRef.current.srcObject = stream;
      }

      // Initialize Socket.IO connection
      initializeSocket();
      
    } catch (error) {
      console.error('Failed to initialize connection:', error);
      setConnectionStatus('disconnected');
    }
  };

  const initializeSocket = () => {
    const backendUrl = import.meta.env.VITE_BACKEND_URL || 'http://localhost:3001';
    const socket = io(backendUrl, {
      timeout: 5000,
      transports: ['websocket', 'polling']
    });
    socketRef.current = socket;
    
    socket.on('connect_error', (error) => {
      console.error('üîå Connection failed:', error);
      setConnectionStatus('disconnected');
      // Fallback to mock interview mode
      setTimeout(() => {
        startMockInterview();
      }, 2000);
    });
    
    socket.on('connect', () => {
      console.log('üîå Connected to backend');
      setConnectionStatus('connected');
      setIsConnected(true);
      
      // Start interview
      socket.emit('start-interview', {
        sessionId,
        candidateName,
        position
      });
    });

    socket.on('ai-question', (data) => {
      handleAIQuestion(data);
    });

    socket.on('response-processed', (data) => {
      handleResponseProcessed(data);
    });
    
    socket.on('video-analysis', (data) => {
      handleVideoAnalysis(data);
    });

    socket.on('interview-complete', (data) => {
      handleInterviewComplete(data);
    });

    socket.on('disconnect', () => {
      console.log('üîå Disconnected from backend');
      setConnectionStatus('disconnected');
      if (currentQuestion < 7 && !interviewEnded) {
        startMockInterview();
      }
    });

    socket.on('error', (error) => {
      console.error('‚ùå Socket error:', error);
      setConnectionStatus('disconnected');
      // Fallback to mock interview
      setTimeout(() => {
        startMockInterview();
      }, 2000);
    });
  };

  const handleAIQuestion = async (data: any) => {
    const { questionIndex, questionText, audioBuffer, isLastQuestion } = data;
    
    setCurrentQuestion(questionIndex);
    setCurrentQuestionText(questionText);
    setAiSpeaking(true);
    
    // Play AI question audio
    if (audioBuffer) {
      try {
        const audioBlob = new Blob([Buffer.from(audioBuffer, 'base64')], { type: 'audio/mpeg' });
        const audioUrl = URL.createObjectURL(audioBlob);
        const audio = new Audio(audioUrl);
        
        audio.onended = () => {
          setAiSpeaking(false);
          startListening();
          URL.revokeObjectURL(audioUrl);
        };
        
        await audio.play();
      } catch (error) {
        console.error('Failed to play question audio:', error);
        setAiSpeaking(false);
        startListening();
      }
    } else {
      // Fallback to text-to-speech
      await speakQuestion(questionText);
      setAiSpeaking(false);
      startListening();
    }
  };

  const handleResponseProcessed = (data: any) => {
    const { questionIndex, transcript, evaluation } = data;
    console.log(`‚úÖ Response ${questionIndex + 1} processed:`, evaluation);
    
    setResponses(prev => [...prev, {
      questionIndex,
      transcript,
      evaluation,
      videoAnalysis
    }]);
    
    setVideoAnalysis(''); // Clear analysis for next question
  };
  
  const handleVideoAnalysis = (data: any) => {
    const { analysis } = data;
    setVideoAnalysis(analysis);
    console.log('üìπ Video analysis:', analysis);
  };

  // Mock interview fallback when backend is not available
  const startMockInterview = () => {
    if (interviewStarted) return; // Prevent multiple starts
    
    console.log('ü§ñ Starting mock interview mode...');
    setConnectionStatus('connected'); // Simulate connection
    setIsConnected(true);
    setCurrentQuestion(0);
    setWaitingForResponse(false);
    setAiSpeaking(false);
    setCandidateResponding(false);
    setInterviewStarted(true);
    setInterviewEnded(false);
    setQuestionAsked(false);
    
    // Start with first question
    setTimeout(() => {
      askMockQuestion(0);
    }, 2000);
  };

  const askMockQuestion = async (questionIndex: number) => {
    if (interviewEnded || questionInProgress) {
      console.log('Interview ended or question in progress, skipping...');
      return;
    }
    
    const mockQuestions = [
      "Hello! I'm your AI interviewer. Please introduce yourself and tell me about your professional background.",
      "What interests you most about this position and why do you think you'd be a good fit?",
      "Can you describe a challenging project you've worked on recently and how you overcame obstacles?",
      "How do you handle working under pressure and tight deadlines?",
      "What are your greatest strengths and how do they apply to this role?",
      "Where do you see yourself in the next 5 years?",
      "Do you have any questions for me about the role or company?"
    ];

    if (questionIndex >= mockQuestions.length) {
      handleMockInterviewComplete();
      return;
    }

    console.log(`üé§ Asking question ${questionIndex + 1}:`, mockQuestions[questionIndex]);
    
    setQuestionInProgress(true);
    setQuestionAsked(true);
    setCurrentQuestion(questionIndex);
    setCurrentQuestionText(mockQuestions[questionIndex]);
    setAiSpeaking(true);
    setWaitingForResponse(false);
    setCandidateResponding(false);
    setWaitingForAnswer(false);
    
    // Use browser TTS for mock questions
    try {
      await speakQuestion(mockQuestions[questionIndex]);
    } catch (error) {
      console.error('TTS failed:', error);
    }
    
    setAiSpeaking(false);
    setWaitingForAnswer(true);
  };

  const handleMockInterviewComplete = () => {
    if (interviewEnded) return;
    
    setInterviewEnded(true);
    setOverallScore(75); // Mock score
    setAiSpeaking(true);
    
    speakQuestion("Thank you for your time! Your interview is now complete. You'll receive your results shortly.")
      .then(() => {
        setAiSpeaking(false);
        setTimeout(() => {
          onComplete(responses);
        }, 2000);
      });
  };

  const handleInterviewComplete = async (data: any) => {
    const { overallScore, responses, completionAudio, completionMessage } = data;
    
    setOverallScore(overallScore);
    setResponses(responses);
    setAiSpeaking(true);
    
    // Play completion message
    if (completionAudio) {
      try {
        const audioBlob = new Blob([Buffer.from(completionAudio, 'base64')], { type: 'audio/mpeg' });
        const audioUrl = URL.createObjectURL(audioBlob);
        const audio = new Audio(audioUrl);
        
        audio.onended = () => {
          setAiSpeaking(false);
          setTimeout(() => {
            onComplete(responses);
          }, 2000);
          URL.revokeObjectURL(audioUrl);
        };
        
        await audio.play();
      } catch (error) {
        console.error('Failed to play completion audio:', error);
        setAiSpeaking(false);
        setTimeout(() => onComplete(responses), 2000);
      }
    } else {
      setAiSpeaking(false);
      setTimeout(() => onComplete(responses), 2000);
    }
  };

  const speakQuestion = async (question: string): Promise<void> => {
    // Use OpenAI TTS or browser speech synthesis
    return new Promise((resolve) => {
      const utterance = new SpeechSynthesisUtterance(question);
      utterance.rate = 0.9;
      utterance.pitch = 1.0;
      utterance.volume = 0.8;
      
      utterance.onend = () => resolve();
      speechSynthesis.speak(utterance);
    });
  };

  const startListening = () => {
    if (candidateResponding || interviewEnded || !waitingForAnswer) {
      console.log('Already recording or interview ended, ignoring start request');
      return;
    }

    console.log('üé§ Starting to record user response...');
    setCandidateResponding(true);
    setIsRecording(true);
    setWaitingForAnswer(false);
    audioChunksRef.current = [];
    videoChunksRef.current = [];
    
    // Start recording candidate audio + video response
    if (videoRef.current?.srcObject) {
      const stream = videoRef.current.srcObject as MediaStream;
      
      // Audio recorder for transcription
      const audioStream = new MediaStream(stream.getAudioTracks());
      const mediaRecorder = new MediaRecorder(audioStream, {
        mimeType: 'audio/webm'
      });
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
        const videoBlob = new Blob(videoChunksRef.current, { type: 'video/webm' });
        processResponse(audioBlob, videoBlob);
      };

      mediaRecorder.start();
      mediaRecorderRef.current = mediaRecorder;
      
      console.log('Started recording audio response...');
    }
  };

  const stopListening = () => {
    if (!candidateResponding) {
      console.log('Not currently recording, ignoring stop request');
      return;
    }

    console.log('üõë Stopping recording...');
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
      console.log('Stopped audio recording');
    }
    setIsRecording(false);
    setCandidateResponding(false);
  };

  const processResponse = async (audioBlob: Blob, videoBlob?: Blob) => {
    console.log('üéµüé• Sending audio + video response to backend...');
    setAiSpeaking(true); // Show processing indicator
    
    // Send audio + video to backend via Socket.IO if connected
    if (socketRef.current && connectionStatus === 'connected') {
      socketRef.current.emit('audio-response', {
        sessionId,
        audioBlob,
        videoBlob: isVideoEnabled ? videoBlob : null,
        questionIndex: currentQuestion
      });
    } else {
      // Mock processing for fallback mode
      console.log('ü§ñ Processing response in mock mode...');
      setTimeout(() => {
        // Store mock response
        setResponses(prev => [...prev, {
          questionIndex: currentQuestion,
          transcript: '[Mock transcript - Response recorded]',
          evaluation: {
            score: Math.floor(Math.random() * 30) + 60, // 60-90 range
            feedback: 'Good response with clear communication.'
          }
        }]);
        
        // Move to next question
        setAiSpeaking(false);
        setQuestionInProgress(false); // Reset question state
        setQuestionAsked(false); // Allow next question
        setTimeout(() => {
          if (currentQuestion < 6) {
            askMockQuestion(currentQuestion + 1);
          } else {
            handleMockInterviewComplete();
          }
        }, 1500);
      }, 2000);
    }
  };
  
  const toggleVideo = () => {
    setIsVideoEnabled(!isVideoEnabled);
    if (videoRef.current?.srcObject) {
      const stream = videoRef.current.srcObject as MediaStream;
      stream.getVideoTracks().forEach(track => {
        track.enabled = !isVideoEnabled;
      });
    }
  };
  
  const startScreenShare = async () => {
    try {
      const screenStream = await navigator.mediaDevices.getDisplayMedia({
        video: true,
        audio: true
      });
      
      if (videoRef.current) {
        videoRef.current.srcObject = screenStream;
      }
      
      setIsScreenSharing(true);
      
      screenStream.getVideoTracks()[0].onended = () => {
        setIsScreenSharing(false);
        // Switch back to camera
        initializeConnection();
      };
    } catch (error) {
      console.error('Screen sharing failed:', error);
    }
  };
  
  const captureFrame = () => {
    if (videoRef.current && canvasRef.current) {
      const canvas = canvasRef.current;
      const ctx = canvas.getContext('2d');
      
      canvas.width = videoRef.current.videoWidth;
      canvas.height = videoRef.current.videoHeight;
      
      ctx?.drawImage(videoRef.current, 0, 0);
      
      // Convert to blob and send for analysis
      canvas.toBlob((blob) => {
        if (blob && socketRef.current) {
          socketRef.current.emit('analyze-frame', {
            sessionId,
            frameBlob: blob,
            questionIndex: currentQuestion
          });
        }
      }, 'image/jpeg', 0.8);
    }
  };

  const endInterview = () => {
    console.log('üîö Ending interview manually...');
    setInterviewEnded(true);
    setAiSpeaking(false);
    setCandidateResponding(false);
    setIsRecording(false);
    setWaitingForAnswer(false);
    setQuestionInProgress(false);
    
    // Stop any ongoing recording
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
    }
    
    // Clean up and return to dashboard
    cleanup();
    setAiSpeaking(true);
    speakQuestion("Thank you for your time! Your interview is now complete. You'll receive your results shortly.")
      .then(() => {
        setAiSpeaking(false);
        setIsConnected(false);
        onComplete(responses);
      });
  };

  const cleanup = () => {
    if (socketRef.current) {
      socketRef.current.disconnect();
    }
    if (mediaRecorderRef.current) {
      mediaRecorderRef.current.stop();
    }
    if (videoRef.current?.srcObject) {
      const stream = videoRef.current.srcObject as MediaStream;
      stream.getTracks().forEach(track => track.stop());
    }
  };

  return (
    <div className="min-h-screen bg-gray-900 flex items-center justify-center p-4">
      <div className="bg-white rounded-lg shadow-xl max-w-4xl w-full overflow-hidden">
        {/* Header */}
        <div className="bg-blue-600 text-white p-4">
          <div className="flex items-center justify-between">
            <div>
              <h1 className="text-xl font-semibold">AI Video Interview</h1>
              <p className="text-blue-100">{candidateName} ‚Ä¢ {position}</p>
            </div>
            <div className="flex items-center space-x-4">
              <div className={`flex items-center space-x-2 ${
                connectionStatus === 'connected' ? 'text-green-300' : 
                connectionStatus === 'connecting' ? 'text-yellow-300' : 'text-red-300'
              }`}>
                <div className={`w-2 h-2 rounded-full ${
                  connectionStatus === 'connected' ? 'bg-green-300' : 
                  connectionStatus === 'connecting' ? 'bg-yellow-300 animate-pulse' : 'bg-red-300'
                }`} />
                <span className="text-sm capitalize">{connectionStatus}</span>
              </div>
              <div className="text-sm">
                Question {currentQuestion + 1} of 7
              </div>
            </div>
          </div>
        </div>

        {/* Video Area */}
        <div className="relative bg-gray-100 aspect-video">
          <video
            ref={videoRef}
            autoPlay
            muted
            className="w-full h-full object-cover"
          />
          
          {/* Hidden canvas for frame capture */}
          <canvas ref={canvasRef} className="hidden" />
          
          {/* AI Avatar Overlay */}
          <div className="absolute top-4 right-4 w-32 h-24 bg-blue-600 rounded-lg flex items-center justify-center">
            <div className="text-white text-center">
              <Users className="w-8 h-8 mx-auto mb-1" />
              <div className="text-xs">AI Interviewer</div>
              {aiSpeaking && (
                <div className="flex justify-center mt-1">
                  <div className="w-1 h-1 bg-white rounded-full animate-pulse mx-0.5"></div>
                  <div className="w-1 h-1 bg-white rounded-full animate-pulse mx-0.5" style={{animationDelay: '0.1s'}}></div>
                  <div className="w-1 h-1 bg-white rounded-full animate-pulse mx-0.5" style={{animationDelay: '0.2s'}}></div>
                </div>
              )}
            </div>
          </div>

          {/* Status Overlays */}
          {aiSpeaking && (
            <div className="absolute bottom-4 left-4 bg-blue-600 text-white px-4 py-2 rounded-lg flex items-center">
              <MessageSquare className="w-4 h-4 mr-2" />
              {candidateResponding ? 'Processing your response...' : 'AI is asking a question...'}
            </div>
          )}

          {waitingForResponse && !candidateResponding && !aiSpeaking && (
            <div className="absolute bottom-4 left-4 bg-green-600 text-white px-4 py-2 rounded-lg flex items-center">
              <Mic className="w-4 h-4 mr-2" />
              {waitingForAnswer ? 'Click "Start Recording" to answer' : 'Preparing next question...'}
            </div>
          )}

          {candidateResponding && (
            <div className="absolute bottom-4 left-4 bg-red-600 text-white px-4 py-2 rounded-lg flex items-center">
              <Mic className="w-4 h-4 mr-2" />
              Recording your response...
            </div>
          )}
          
          {/* Video Analysis Display */}
          {videoAnalysis && (
            <div className="absolute top-16 right-4 bg-purple-600 text-white px-4 py-2 rounded-lg max-w-xs">
              <div className="flex items-center mb-1">
                <Eye className="w-4 h-4 mr-2" />
                <span className="text-xs font-medium">AI Visual Analysis</span>
              </div>
              <p className="text-xs">{videoAnalysis}</p>
            </div>
          )}

          {/* Current Question Display */}
          {currentQuestionText && !candidateResponding && (
            <div className="absolute bottom-16 left-4 right-4 bg-white bg-opacity-90 p-4 rounded-lg">
              <p className="text-sm text-gray-800">
                <strong>Current Question:</strong> {currentQuestionText}
              </p>
            </div>
          )}

          {!isConnected && connectionStatus === 'connecting' && (
            <div className="absolute inset-0 bg-black bg-opacity-50 flex items-center justify-center">
              <div className="bg-white rounded-lg p-6 text-center">
                <div className="w-12 h-12 border-4 border-blue-600 border-t-transparent rounded-full animate-spin mx-auto mb-4"></div>
                <h2 className="text-lg font-semibold text-gray-800 mb-2">Connecting to AI Interviewer</h2>
                <p className="text-gray-600">Please wait while we set up your video interview...</p>
              </div>
            </div>
          )}

          {connectionStatus === 'disconnected' && !isConnected && (
            <div className="absolute inset-0 bg-black bg-opacity-50 flex items-center justify-center">
              <div className="bg-white rounded-lg p-6 text-center max-w-md">
                <div className="w-12 h-12 bg-yellow-500 rounded-full flex items-center justify-center mx-auto mb-4">
                  <span className="text-white text-xl">‚ö†Ô∏è</span>
                </div>
                <h2 className="text-lg font-semibold text-gray-800 mb-2">Connection Issue</h2>
                <p className="text-gray-600 mb-4">
                  Unable to connect to the AI interviewer backend. 
                  Switching to offline interview mode...
                </p>
                <div className="text-sm text-blue-600">
                  Starting mock interview in 3 seconds...
                </div>
              </div>
            </div>
          )}
        </div>

        {/* Controls */}
        <div className="p-4 bg-gray-50 flex items-center justify-between">
          <div className="flex items-center space-x-4">
            <button 
              onClick={toggleVideo}
              className={`p-2 rounded-full hover:bg-gray-300 ${
                isVideoEnabled ? 'bg-blue-200' : 'bg-gray-200'
              }`}
            >
              {isVideoEnabled ? <Camera className="w-5 h-5" /> : <CameraOff className="w-5 h-5" />}
            </button>
            
            {waitingForAnswer && !candidateResponding && (
              <button 
                onClick={startListening}
                disabled={interviewEnded}
                className="bg-green-600 hover:bg-green-700 text-white px-4 py-2 rounded-lg flex items-center"
              >
                <Mic className="w-4 h-4 mr-2" />
                Start Recording
              </button>
            )}
            
            {candidateResponding && (
              <button 
                onClick={stopListening}
                disabled={interviewEnded}
                className="bg-red-600 hover:bg-red-700 text-white px-4 py-2 rounded-lg flex items-center"
              >
                <MicOff className="w-4 h-4 mr-2" />
                Stop Recording
              </button>
            )}
            
            <button 
              onClick={startScreenShare}
              disabled={isScreenSharing || interviewEnded}
              className={`p-2 rounded-full hover:bg-gray-300 ${
                isScreenSharing ? 'bg-blue-200' : 'bg-gray-200'
              }`}
            >
              <Monitor className="w-5 h-5" />
            </button>
          </div>
          
          <div className="text-center">
            <div className="text-sm text-gray-600">
              {aiSpeaking ? 'AI is speaking...' :
               candidateResponding ? 'Recording your response...' : 
               waitingForAnswer ? 'Ready for your answer' :
               'Processing...'}
            </div>
            {overallScore && (
              <div className="text-lg font-bold text-green-600">
                Final Score: {overallScore}%
              </div>
            )}
          </div>
          
          <button 
            onClick={endInterview}
            className="bg-red-600 hover:bg-red-700 text-white px-4 py-2 rounded-lg flex items-center"
          >
            <PhoneOff className="w-4 h-4 mr-2" />
            End Interview
          </button>
        </div>

        {/* Progress Bar */}
        <div className="h-1 bg-gray-200">
          <div 
            className="h-full bg-blue-600 transition-all duration-300"
            style={{ width: `${((currentQuestion + 1) / 7) * 100}%` }}
          />
        </div>
      </div>
    </div>
  );
};